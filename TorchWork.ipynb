{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intro to Torch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2,3)\n",
    "torch.rand(shape)\n",
    "tensor = torch.ones(2,3)\n",
    "tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "  print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.6936e-01, -4.7470e-01, -7.7365e-01, -1.8096e+00, -9.2946e-01,\n",
       "         -2.1604e-01, -5.3495e-01,  6.4181e-01,  6.3297e-01, -6.4690e-01,\n",
       "         -5.7755e-01, -5.1415e-01, -1.0975e-02, -7.6475e-01, -1.0861e+00,\n",
       "         -5.7404e-01, -7.0902e-01,  2.6921e-02, -3.7373e-01, -1.4492e-01,\n",
       "         -1.5400e+00, -7.7540e-01, -1.4056e+00, -7.2108e-02, -9.6053e-01,\n",
       "         -1.2402e+00, -1.0737e+00, -1.3540e+00, -9.8589e-01, -2.5344e-01,\n",
       "         -1.0027e+00, -6.6669e-01, -5.7755e-01, -4.7633e-01, -4.6286e-01,\n",
       "         -4.7300e-01,  5.6732e-01, -7.1667e-01, -3.2017e-01,  1.4508e-01,\n",
       "         -9.1095e-01, -9.6330e-01, -9.3278e-01, -2.0563e-01, -8.4864e-01,\n",
       "         -4.0223e-01, -8.3286e-01, -5.0122e-01, -1.3263e+00, -1.1246e+00,\n",
       "         -6.8112e-01,  2.8575e-01, -5.1178e-01, -8.3557e-01, -1.9911e-01,\n",
       "         -1.2955e+00, -1.5986e-01, -1.7336e+00, -7.4570e-01, -4.9299e-01,\n",
       "          8.0457e-01,  1.2843e-01, -2.0775e-01,  1.9774e-01, -9.2462e-01,\n",
       "         -2.5730e-01, -3.0241e-01, -2.1662e-01, -7.6678e-01, -1.3429e+00,\n",
       "         -1.6946e+00,  8.3804e-02, -1.3588e+00, -9.1184e-02, -9.6783e-01,\n",
       "         -1.2511e+00,  6.9919e-02, -6.7911e-01,  4.4701e-01,  7.1536e-02,\n",
       "         -6.9942e-01, -1.5585e+00, -8.8213e-02, -5.9214e-01, -2.2981e-01,\n",
       "          6.3777e-02,  2.0826e-02,  2.4618e-01,  1.9440e-01, -6.8631e-01,\n",
       "         -9.4055e-01, -9.3455e-01, -1.5482e+00, -3.6310e-01,  6.3799e-02,\n",
       "         -1.9203e+00, -6.2427e-01, -3.9672e-01, -1.4318e+00, -1.6215e-01,\n",
       "         -1.2009e+00, -1.2965e+00, -9.0101e-01, -5.3971e-01, -4.4616e-01,\n",
       "         -6.6376e-01, -5.8276e-01, -1.3913e+00, -1.1788e+00, -1.4483e+00,\n",
       "         -1.0209e+00, -8.5341e-01,  1.0870e+00,  5.5119e-01,  4.7159e-01,\n",
       "         -8.8310e-01, -8.7634e-01, -1.6801e-01,  6.7471e-01, -1.7667e-01,\n",
       "         -7.3162e-01,  1.4830e-01,  9.1173e-02,  3.5851e-02,  8.2686e-01,\n",
       "         -2.0335e-01,  1.4819e-01, -1.2795e+00, -1.4041e+00, -1.0997e+00,\n",
       "         -1.3470e+00, -1.7871e+00, -1.2512e+00, -1.5597e+00, -8.9447e-01,\n",
       "         -1.2547e+00, -9.7759e-01, -1.2492e+00, -1.3612e+00, -1.4126e+00,\n",
       "         -1.2391e+00, -1.5399e+00, -1.9371e+00, -1.6934e+00, -4.9665e-01,\n",
       "         -4.9824e-01, -1.0633e+00, -1.8696e+00, -1.3067e+00, -1.3902e+00,\n",
       "          4.5515e-01,  1.6513e+00, -9.9457e-01, -4.9278e-01, -3.3931e-02,\n",
       "         -1.3636e-02, -4.9389e-01, -2.8391e-01,  4.3806e-02,  3.1816e-02,\n",
       "          1.5785e-01,  4.4673e-01,  1.3522e-01,  4.3364e-01,  3.2796e-01,\n",
       "         -6.5940e-01, -3.4301e-01, -5.9855e-01,  4.2405e-01, -5.0110e-01,\n",
       "          8.7358e-02,  6.1864e-01,  1.3402e-01,  2.1879e-01,  1.6858e-01,\n",
       "         -7.4062e-01, -3.1727e-01, -6.3860e-02,  2.4486e-01,  4.9836e-01,\n",
       "          5.1781e-01,  8.6776e-02,  6.5679e-01,  9.2205e-02,  5.3919e-01,\n",
       "          6.4694e-01,  8.8400e-01,  1.8319e-01,  4.0399e-01,  9.4620e-01,\n",
       "         -5.1599e-01,  4.9850e-01,  4.3288e-01,  6.5090e-01, -5.9099e-01,\n",
       "          8.2679e-01, -1.1534e-03,  9.2319e-02,  1.0382e-01,  6.9739e-01,\n",
       "          7.6493e-03,  3.0239e-01,  7.5287e-01,  8.5643e-01, -2.4521e-02,\n",
       "          7.5967e-03, -1.0708e-01,  2.3984e-01,  8.7612e-01,  2.7974e-01,\n",
       "          3.5695e-02,  3.2997e-01,  2.7647e-01, -2.4938e-01, -1.1314e-01,\n",
       "          9.7618e-02, -2.9193e-01,  2.3837e-01, -4.9305e-01,  6.2194e-01,\n",
       "          1.8414e-01,  7.5920e-02, -1.9366e-01,  7.2289e-01,  4.6941e-02,\n",
       "          4.1420e-01,  2.5961e-01,  7.1852e-01, -1.9680e-01,  1.3334e-02,\n",
       "         -8.9746e-02,  5.1160e-01,  3.4803e-01, -4.8036e-02,  5.2381e-01,\n",
       "          7.4441e-01,  3.8446e-01,  1.8825e-01,  6.9829e-01, -1.2326e-01,\n",
       "          2.4523e-01, -1.4824e-01,  4.0878e-01,  2.0497e-01, -2.8750e-01,\n",
       "          6.2845e-01,  2.5530e-01, -2.6691e-02,  7.9735e-01,  2.5708e-01,\n",
       "          5.2771e-01,  5.6088e-01, -7.6053e-01,  4.9678e-01,  8.7444e-01,\n",
       "         -5.4752e-01,  3.4856e-01,  1.4668e-01,  9.2878e-02,  4.6065e-01,\n",
       "          1.5803e-02, -4.7335e-01, -3.1650e-01,  5.5744e-01,  7.6667e-01,\n",
       "          7.5609e-01,  3.5636e-01,  6.1263e-01, -1.0305e-02, -4.4041e-01,\n",
       "         -6.9925e-01, -1.0131e+00, -6.4284e-01,  7.0907e-01, -1.1370e+00,\n",
       "         -9.0461e-01, -1.1573e+00, -7.4960e-01, -1.2020e+00, -4.4316e-01,\n",
       "         -3.3768e-01,  7.1881e-01,  5.8748e-01, -1.1550e-01,  2.5002e-01,\n",
       "          7.7746e-01, -4.8436e-01, -3.6313e-01, -6.4568e-01, -1.6601e+00,\n",
       "         -1.0953e+00, -1.4561e+00, -5.6815e-01, -9.4079e-01, -1.1466e+00,\n",
       "         -1.0244e+00, -9.0336e-01, -1.6009e+00, -5.8479e-01, -2.5482e-01,\n",
       "         -2.2578e+00, -5.1093e-01, -5.2586e-01, -3.6259e-01, -9.5675e-01,\n",
       "         -7.5322e-01,  4.1384e-01, -5.9533e-01, -1.2868e+00, -3.0128e-01,\n",
       "          2.4239e-01, -2.0837e-01, -4.2224e-01,  1.5218e-01,  7.4465e-01,\n",
       "         -7.2273e-01, -7.8175e-01, -1.0207e+00, -1.3382e+00, -6.4820e-01,\n",
       "         -1.6743e+00, -1.1789e+00, -1.5672e+00, -1.5589e+00, -1.6410e+00,\n",
       "         -1.6566e+00, -1.2917e+00, -2.1154e-01,  3.7089e-02, -5.8822e-01,\n",
       "         -2.9110e-02, -1.6038e-01, -7.4055e-03,  4.1349e-01, -4.9557e-02,\n",
       "         -9.8071e-01, -1.5833e+00,  3.5293e-01,  9.4520e-01, -1.2644e+00,\n",
       "         -6.5356e-01,  8.2314e-01, -2.9068e-01, -1.4656e+00, -7.6625e-01,\n",
       "          9.9241e-01, -6.9995e-01, -1.4127e+00, -1.8968e-01, -1.6148e+00,\n",
       "         -1.2061e+00, -2.2806e+00, -1.3475e+00, -9.4843e-01, -6.8926e-01,\n",
       "          4.5721e-01,  9.7135e-01, -1.9483e-01,  2.2256e-01,  2.1788e-01,\n",
       "         -3.8045e-01,  5.3691e-01, -9.5933e-02,  2.3959e-01, -4.6816e-01,\n",
       "         -6.1072e-01, -1.3097e+00, -5.4580e-01, -9.5361e-01, -8.7079e-01,\n",
       "         -4.3927e-01, -3.9115e-01, -1.3791e-01,  1.7539e-01, -8.3834e-02,\n",
       "         -8.1322e-01, -1.0196e+00,  4.5713e-01, -5.2092e-02, -4.1529e-01,\n",
       "          5.3054e-01, -4.8602e-01, -1.8589e-01, -7.5069e-01, -6.9268e-01,\n",
       "         -8.1505e-01, -1.4601e+00, -1.0100e+00, -9.2598e-01, -5.3891e-01,\n",
       "          6.6714e-01, -2.3503e-02, -1.3840e+00, -1.6179e+00, -4.3541e-01,\n",
       "          3.8634e-01, -1.0167e+00, -4.6618e-01,  2.1889e-01,  6.1097e-02,\n",
       "         -6.5824e-01,  9.8174e-01,  1.2367e-01, -2.1028e+00, -2.0480e+00,\n",
       "         -8.2443e-01,  2.2776e-01, -1.9753e-01, -2.3902e-01,  1.4157e+00,\n",
       "         -8.4124e-02,  3.4846e-01,  2.1753e+00,  6.4454e-01,  5.2758e-01,\n",
       "          8.9817e-01, -3.6953e-01,  1.7391e-01,  3.9130e-01,  1.1088e+00,\n",
       "          9.7217e-01,  1.2378e+00, -1.3347e-01,  2.8889e-01,  2.3626e-01,\n",
       "         -9.1473e-01,  1.2315e-01,  1.4730e+00,  2.0448e+00,  5.8128e-02,\n",
       "         -6.1177e-01, -1.4620e-01,  5.2055e-01,  6.3477e-01,  2.6442e-01,\n",
       "          7.4798e-01, -4.8548e-01, -5.3984e-01,  4.6778e-01,  5.0693e-01,\n",
       "          1.0389e+00,  6.6196e-01,  3.4415e-01, -4.2727e-01, -1.7386e-01,\n",
       "         -3.7433e-03,  6.3009e-01,  1.1621e+00,  1.2405e+00, -4.9599e-01,\n",
       "         -3.4295e-01,  6.1384e-01,  3.3078e-01, -3.9608e-01, -1.7911e-01,\n",
       "          9.1928e-01,  1.4889e+00,  1.1076e+00, -3.6750e-01,  3.2754e-01,\n",
       "         -6.5816e-01,  6.1037e-01,  1.5277e+00,  2.4843e+00,  1.1122e+00,\n",
       "         -7.6494e-01, -1.3791e+00, -4.4961e-02,  5.1523e-02,  1.8903e+00,\n",
       "          1.1848e+00,  5.3655e-01,  3.4178e-01,  2.4804e-01, -3.4323e-01,\n",
       "          1.2040e-01,  3.0173e-01,  5.2665e-01,  7.2740e-01,  3.0076e-01,\n",
       "         -1.1577e-01, -6.7082e-02, -1.8683e-02, -9.0335e-01, -1.4908e+00,\n",
       "         -3.2877e-01, -1.9786e-01,  1.2175e+00,  1.6694e+00,  1.3376e+00,\n",
       "          6.0745e-01,  7.6912e-01,  7.5887e-01, -1.0972e+00,  1.5782e+00,\n",
       "         -1.0346e+00,  9.4161e-02, -2.7928e-01, -1.0904e-01,  1.4648e+00,\n",
       "         -1.6501e+00,  5.1784e-01,  1.2947e+00,  3.7632e-01,  1.1355e+00,\n",
       "          1.0417e+00,  1.0137e+00,  6.1089e-01,  3.1453e-01,  1.8215e-01,\n",
       "         -1.1119e+00, -1.0390e+00,  8.0349e-01,  2.4061e-01,  1.0608e+00,\n",
       "          1.4686e+00,  3.8419e-01,  2.1598e-01,  1.1805e+00,  4.4954e-01,\n",
       "         -6.2206e-01,  3.1183e-01,  8.6862e-01,  1.8879e+00,  2.5073e-01,\n",
       "         -1.0765e+00, -8.8854e-03, -5.7210e-01,  4.1677e-02,  1.1183e-01,\n",
       "          8.0050e-01,  2.2067e-01,  2.8910e-01, -7.3226e-01,  1.9453e-01,\n",
       "         -5.1211e-01, -2.8749e-01, -4.1259e-01,  4.6713e-01,  1.3473e+00,\n",
       "         -8.2871e-01,  1.9725e+00,  1.2769e+00,  6.5787e-01,  6.2799e-01,\n",
       "          8.8325e-01,  6.5154e-01, -1.9186e+00, -1.4429e+00, -4.2453e-02,\n",
       "         -7.6270e-01,  1.3802e-01,  7.2937e-01, -2.1246e-01, -1.4915e+00,\n",
       "         -6.0390e-01,  4.9341e-01,  2.1831e-01,  1.1761e+00,  1.1296e+00,\n",
       "         -1.8781e-01,  2.6303e-01,  1.0229e+00,  9.2539e-02, -9.7411e-01,\n",
       "         -1.0620e+00,  1.8947e-02,  1.0220e+00,  5.0383e-01, -4.5043e-01,\n",
       "          7.5911e-01,  8.2551e-02,  1.1170e+00, -1.0162e+00,  5.4939e-01,\n",
       "         -1.9453e-01, -6.2902e-01,  1.2710e+00,  2.4462e-01,  3.4566e-01,\n",
       "          2.4548e-01, -1.4790e-01,  8.8509e-01,  8.2335e-01,  6.9047e-01,\n",
       "          8.2325e-01, -5.1763e-01,  1.6357e+00,  1.1299e+00,  9.5565e-01,\n",
       "         -5.8111e-01,  4.3584e-01, -5.2551e-01,  4.6484e-01,  2.6952e-01,\n",
       "         -8.0478e-01,  1.1357e+00, -3.8990e-01, -6.9245e-01,  1.2073e+00,\n",
       "          2.2579e+00, -2.1759e-03,  5.4400e-02, -3.3830e-01,  6.6448e-01,\n",
       "          1.7298e-01,  1.0104e+00, -6.5920e-01,  3.9069e-01, -2.3748e-01,\n",
       "          7.6229e-01,  7.5639e-01, -2.3555e-01,  2.8471e-01,  1.9621e-01,\n",
       "          2.1683e-02,  1.0292e+00,  5.9939e-01,  2.0282e+00,  1.1206e+00,\n",
       "          1.1897e+00,  5.6121e-01,  1.7436e-01,  4.4821e-01,  1.1064e-01,\n",
       "         -1.0482e+00,  1.0450e+00, -6.2908e-01, -1.1237e+00,  1.9018e-01,\n",
       "          1.2173e-01,  9.4901e-01,  8.6877e-01,  1.1622e+00, -1.0567e-01,\n",
       "          4.0609e-01,  1.1860e+00,  8.4777e-01,  5.0194e-01,  9.8989e-02,\n",
       "         -1.3770e+00,  1.2145e+00,  3.3200e-01,  1.3260e+00,  9.6523e-01,\n",
       "         -5.5525e-01,  8.1003e-01,  1.3048e-01, -6.8771e-01, -1.7711e+00,\n",
       "          9.9792e-01,  2.8210e-01,  4.9715e-01,  8.2257e-01, -1.4207e-01,\n",
       "          7.3829e-01,  1.6314e-01,  2.0138e-02, -8.5555e-02,  5.5355e-01,\n",
       "         -1.9555e-01, -1.2394e+00, -2.7431e-01, -5.7571e-01,  9.4990e-01,\n",
       "          9.1293e-02,  1.2364e+00,  5.3338e-01, -6.5745e-01, -6.1073e-02,\n",
       "          3.2012e-01, -1.9613e-01, -7.2516e-02,  2.8580e-01,  1.2228e+00,\n",
       "         -8.0388e-01,  1.6564e+00,  1.4256e+00,  7.1841e-01,  3.5370e-01,\n",
       "          4.2476e-01,  6.1519e-01, -6.2476e-01,  7.5520e-01,  7.3249e-01,\n",
       "         -1.5128e+00, -1.7207e-01, -1.0169e+00, -3.9258e-01, -7.4085e-01,\n",
       "         -2.8264e-01,  6.2866e-01,  7.5072e-01,  5.7341e-01, -7.4105e-01,\n",
       "          1.0688e+00,  1.8938e+00, -2.8501e-02, -3.6291e-01,  5.7944e-01,\n",
       "          1.7527e+00, -3.8755e-01,  2.1666e-02,  3.2325e-01,  8.7045e-01,\n",
       "         -5.4728e-01, -1.2378e-02,  3.2199e-02,  8.5665e-01,  4.4812e-01,\n",
       "          1.0427e+00,  8.2204e-01,  1.0710e-01, -6.3213e-01,  2.9965e-01,\n",
       "         -6.6413e-01,  5.6374e-01, -8.8606e-01, -2.3008e-01,  5.1997e-01,\n",
       "          2.1266e-01,  4.6433e-01,  1.3319e+00,  3.3672e-01, -5.3496e-01,\n",
       "          1.1951e+00, -9.8975e-01, -3.8176e-01,  1.5684e+00, -4.2198e-01,\n",
       "          2.8329e-01,  2.2150e+00, -1.1265e+00,  1.8394e+00, -1.5578e+00,\n",
       "          2.3837e-01, -2.6382e-01,  7.2369e-01,  1.3760e+00,  3.1678e-01,\n",
       "          8.5231e-01,  2.0137e-01,  1.5572e-01,  3.9739e-01,  5.3677e-01,\n",
       "          1.5180e-01,  2.2337e-01,  5.3342e-01,  8.2697e-01,  1.5876e+00,\n",
       "          5.0872e-01, -4.0776e-01,  2.0009e-01,  5.3923e-01,  8.6053e-01,\n",
       "         -3.7896e-01,  1.0735e+00, -1.8701e-01,  1.1890e+00, -5.0630e-01,\n",
       "         -9.9470e-03,  7.9589e-01,  6.0298e-01,  2.4182e-01,  1.1866e+00,\n",
       "          1.0545e+00,  6.4555e-01,  6.9080e-01, -3.5418e-01,  1.2949e+00,\n",
       "          4.2090e-01,  4.4292e-01,  1.4711e+00,  6.1612e-01,  8.8170e-01,\n",
       "          6.2241e-01,  4.6047e-01,  6.1721e-01,  1.8244e+00, -7.4648e-01,\n",
       "         -1.0098e+00, -8.7183e-01,  9.6936e-01,  1.1066e+00,  1.5781e+00,\n",
       "          1.8148e-01,  4.0002e-01,  1.5097e+00,  3.6171e-01,  4.4045e-02,\n",
       "          8.8270e-01,  1.1958e+00,  1.6730e+00,  1.1724e+00,  2.4075e-01,\n",
       "          2.3707e-01,  7.0889e-01,  7.0302e-01, -8.2857e-01,  5.7244e-01,\n",
       "         -8.0294e-01,  4.3331e-01, -1.1336e+00, -1.0019e+00,  1.1118e+00,\n",
       "          1.2141e+00,  1.5896e-01,  6.8297e-01,  1.3621e+00,  7.8418e-02,\n",
       "         -5.8150e-01,  1.1779e+00, -5.5186e-01,  1.8239e+00, -1.0212e+00,\n",
       "         -1.4318e-01,  5.9066e-01, -1.2744e+00,  1.6466e+00,  5.7250e-01,\n",
       "         -1.7504e+00, -9.1964e-01,  7.7833e-01,  9.0044e-01,  9.0942e-01,\n",
       "         -8.7702e-01,  4.4629e-01,  1.2489e+00,  1.0814e+00, -4.5738e-01,\n",
       "          1.3005e+00,  3.6680e-01, -6.3472e-01, -1.1818e+00,  1.3257e-01,\n",
       "          6.0217e-01,  1.7749e+00,  1.8029e+00,  1.2870e+00, -4.6898e-01,\n",
       "          1.7492e+00,  8.5909e-02,  6.1393e-01,  5.8587e-01,  5.4298e-01,\n",
       "          1.7306e+00,  9.4224e-01, -3.7473e-01,  1.5364e-01,  8.5566e-01,\n",
       "          1.3972e+00,  1.4273e+00,  1.6487e+00, -5.6617e-01, -1.9316e-01,\n",
       "          8.9300e-01, -5.3471e-01,  1.9453e-02, -1.5324e-01,  8.7691e-01,\n",
       "          2.1655e-01,  1.4059e+00,  1.2417e+00, -7.9365e-02, -4.4339e-01,\n",
       "          4.9673e-01,  4.2519e-02, -4.4061e-01,  1.7166e+00, -5.7286e-01,\n",
       "          7.8202e-01, -1.4216e+00,  1.0575e+00, -1.0342e+00, -2.1880e+00,\n",
       "          3.5179e-01,  1.4355e+00,  1.7233e-02, -2.8098e-01,  1.8592e+00,\n",
       "          1.3221e+00, -2.5616e-01,  9.7795e-01,  1.5254e+00,  2.2095e-01,\n",
       "          3.8372e-02, -1.9252e-01,  1.6165e-01, -1.2362e+00,  2.7597e-01,\n",
       "         -5.1968e-01,  3.0353e-01,  1.0994e+00,  2.1514e-01, -7.0388e-01,\n",
       "         -8.1928e-01,  1.1167e+00,  6.5291e-01,  1.9205e+00,  1.7898e+00,\n",
       "         -9.0687e-01, -5.2945e-01,  1.5769e+00,  8.9767e-01,  9.7609e-01,\n",
       "         -7.0379e-02, -4.6334e-01,  1.3960e+00, -9.6486e-01,  1.0759e+00,\n",
       "          1.3194e+00,  1.1353e+00,  6.6759e-01, -3.1036e-01, -1.7053e+00,\n",
       "         -2.3097e-01, -2.5863e-03,  1.5136e-01,  5.3221e-01,  2.6907e-01,\n",
       "          1.0070e-02,  9.4441e-01, -8.5349e-01,  7.4427e-01, -3.7265e-01,\n",
       "         -7.2761e-01, -1.1335e+00, -3.7810e-01,  2.0837e-01,  1.5036e+00,\n",
       "         -2.8401e-01,  3.1031e-01,  5.1555e-01, -1.5526e+00, -9.9561e-02,\n",
       "         -6.4176e-01,  5.5984e-01,  2.7332e-01,  7.3792e-03, -2.5020e-02,\n",
       "         -3.2880e-01, -8.4925e-01,  4.9368e-02,  1.7192e-01, -6.6257e-01,\n",
       "         -9.3312e-01, -1.2143e+00,  6.5001e-01,  6.9722e-01, -4.4439e-01,\n",
       "          1.1955e-01, -5.3451e-01, -3.8774e-01,  1.4140e-01,  6.2916e-01,\n",
       "         -3.4926e-01, -1.5286e-01, -4.2013e-01, -9.2522e-02, -9.1238e-01,\n",
       "          5.1907e-01,  4.0188e-01, -3.2723e-01, -7.5233e-01, -1.3231e+00,\n",
       "         -1.9563e-01,  7.1068e-01, -5.6337e-01,  7.8053e-01, -1.0683e-01,\n",
       "         -1.8055e-01,  9.2426e-01, -6.4709e-01, -4.0259e-01, -1.9925e+00,\n",
       "          1.0324e+00, -1.7116e+00,  5.3360e-01,  2.8199e-01, -7.8358e-01,\n",
       "         -5.3002e-01, -2.1775e-01,  5.2443e-01, -5.8550e-01, -1.0135e+00,\n",
       "         -1.2842e+00, -2.5946e+00,  1.4586e+00, -1.4715e-01, -7.3543e-01,\n",
       "         -3.1139e-02, -1.1634e+00, -7.3895e-01, -1.9904e+00, -5.8132e-01,\n",
       "         -4.4265e-01,  1.2770e-01, -3.5303e-01,  1.0764e+00,  9.3232e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(data)\n",
    "loss = (prediction - labels).sum()\n",
    "loss.backward() \n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -9.9961,  -9.7858,  -9.8884, -11.0517, -10.3028,  -9.8727,  -9.9665,\n",
       "          -8.6391,  -9.0389, -10.1599, -10.2207,  -9.9898,  -9.4981, -10.1876,\n",
       "         -10.3829, -10.0036, -10.4114,  -9.3333,  -9.6983,  -9.4883, -10.8499,\n",
       "         -10.2680, -10.8709,  -9.4191, -10.1939, -10.3498, -10.0295, -10.3315,\n",
       "         -10.0532,  -9.6892, -10.1193, -10.0840,  -9.8188,  -9.8327,  -9.7211,\n",
       "          -9.7313,  -8.6692,  -9.8922,  -9.5384,  -8.9775, -10.0276, -10.1461,\n",
       "         -10.1908,  -9.5130,  -9.9184,  -9.6942,  -9.9121,  -9.7771, -10.5993,\n",
       "         -10.3735,  -9.9601,  -9.1004,  -9.6527,  -9.7040,  -9.5601, -10.4115,\n",
       "          -9.5276, -10.6021,  -9.7510,  -9.8299,  -8.4517,  -9.2089,  -9.5766,\n",
       "          -9.2477, -10.2333,  -9.5723,  -9.7039,  -9.7148, -10.2486, -10.4907,\n",
       "         -11.0234,  -8.6975, -10.6354,  -9.4622, -10.2414, -10.4325,  -9.0119,\n",
       "          -9.8618,  -8.8631,  -9.0092, -10.3223, -10.9597,  -9.5410, -10.1833,\n",
       "          -9.8941,  -9.5399,  -9.3954,  -8.9407,  -9.0135,  -9.8469, -10.1622,\n",
       "         -10.5180, -10.9127,  -9.7789,  -9.2889, -11.3416,  -9.9255,  -9.6891,\n",
       "         -10.9314,  -9.7364, -10.6936, -10.7779, -10.5437,  -9.6623,  -9.6767,\n",
       "          -9.9671,  -9.9568, -10.4289, -10.4621, -11.1056, -10.5323,  -9.9669,\n",
       "          -8.1199,  -8.8887,  -9.0352, -10.4362, -10.3458,  -9.5974,  -8.7265,\n",
       "          -9.5043,  -9.9948,  -9.1916,  -8.7266,  -9.1941,  -7.9250,  -9.2302,\n",
       "          -9.0171, -10.6443, -10.6594, -10.5050, -10.7026, -10.8029, -10.2053,\n",
       "         -10.9588,  -9.8416, -10.8324, -10.2943, -10.6936, -10.8541, -10.6316,\n",
       "         -10.6062, -10.9976, -11.6369, -11.0264,  -9.8048,  -9.7243, -10.4659,\n",
       "         -11.3626, -10.5796, -10.7350,  -9.0794,  -7.8800, -10.5237,  -9.9147,\n",
       "          -9.3777,  -9.1087,  -9.9262,  -9.5708,  -9.1752,  -9.3044,  -9.4775,\n",
       "          -8.8814,  -9.5012,  -8.8922,  -9.1343,  -9.9797,  -9.9169, -10.1980,\n",
       "          -9.0532,  -9.9421,  -9.4106,  -8.6024,  -9.1276,  -9.2064,  -9.4814,\n",
       "         -10.1834,  -9.6248,  -9.3837,  -8.9028,  -8.9849,  -9.1216,  -9.5947,\n",
       "          -8.6278,  -9.3643,  -8.6695,  -8.6478,  -8.5588,  -8.9064,  -9.2097,\n",
       "          -8.7713,  -9.7406,  -8.9811,  -9.1312,  -8.5960, -10.0099,  -8.5411,\n",
       "          -9.2827,  -9.4585,  -9.2326,  -8.8523,  -9.3914,  -8.7749,  -8.7405,\n",
       "          -8.5311,  -9.1724,  -9.4795,  -9.6715,  -9.0822,  -8.6811,  -9.2798,\n",
       "          -9.5510,  -9.0593,  -9.0203,  -9.5523,  -9.4965,  -9.3143,  -9.5130,\n",
       "          -9.2016,  -9.9513,  -8.8913,  -9.4100,  -9.8432,  -9.7726,  -8.9424,\n",
       "          -9.5185,  -9.3149,  -9.2600,  -8.9417,  -9.7813,  -9.3495,  -9.3316,\n",
       "          -9.0188,  -9.1388,  -9.5913,  -8.7487,  -8.7872,  -8.9522,  -8.9091,\n",
       "          -8.9500,  -9.7232,  -9.0531,  -9.7166,  -9.0868,  -9.2918, -10.0440,\n",
       "          -9.0305,  -9.1748,  -9.7275,  -8.9861,  -9.4188,  -9.2566,  -8.7526,\n",
       "         -10.2989,  -9.0103,  -8.7222, -10.3219,  -9.4496,  -9.3701,  -9.4526,\n",
       "          -9.3023,  -9.7941, -10.0927,  -9.8528,  -9.0740,  -8.7664,  -8.7096,\n",
       "          -9.2651,  -9.0269,  -9.5058, -10.1318, -10.5772, -10.8499, -10.0866,\n",
       "          -9.2015, -10.9339, -10.6221, -10.7145, -10.4323, -11.0503, -10.0932,\n",
       "          -9.9593,  -8.6915,  -8.7727,  -9.6761,  -9.4655,  -8.6598, -10.2260,\n",
       "         -10.0737, -10.2557, -11.1652, -10.6816, -11.0957,  -9.9513, -10.5180,\n",
       "         -10.8434, -10.7560, -10.4763, -11.2742,  -9.9507,  -9.7554, -11.2140,\n",
       "          -9.7819,  -9.5017,  -9.4514, -10.2905,  -9.6953,  -8.8868,  -9.8048,\n",
       "         -10.2913,  -9.4126,  -8.8203,  -9.3704,  -9.4339,  -8.9871,  -8.5812,\n",
       "          -9.8074,  -9.9211, -10.2183, -10.5934,  -9.8464, -10.8622, -10.4712,\n",
       "         -10.9987, -11.0050, -10.9986, -11.0899, -10.6903,  -9.3691,  -9.3757,\n",
       "         -10.0106,  -9.5371,  -9.8054,  -9.4709,  -9.1404,  -9.8098, -10.3604,\n",
       "         -10.9343,  -8.9659,  -8.6058, -10.8317,  -9.8540,  -8.9232,  -9.9724,\n",
       "         -10.8527, -10.0265,  -8.6484, -10.0112, -11.0939,  -9.6334, -11.0255,\n",
       "         -10.6836, -11.6953, -10.9912, -10.4946, -10.4976,  -9.3144,  -8.5920,\n",
       "          -9.7120,  -9.2588,  -9.1728,  -9.7562,  -8.9996,  -9.4972,  -8.9856,\n",
       "         -10.1071, -10.1345, -11.0681, -10.2097, -10.7845, -10.5412, -10.0104,\n",
       "         -10.0746, -10.0134,  -9.6316,  -9.7649, -10.4321, -10.7678,  -9.1568,\n",
       "          -9.8735, -10.1681,  -9.1099, -10.2446,  -9.7350, -10.1737, -10.3809,\n",
       "         -10.2064, -10.8001, -10.9135, -10.7374,  -9.6039,  -8.7242,  -8.9957,\n",
       "         -10.8642, -11.1100,  -9.6079,  -8.7799, -10.4139, -10.0195,  -8.8392,\n",
       "          -9.3099, -10.0073,  -8.2722,  -9.0473, -11.3345, -11.1267,  -9.9922,\n",
       "          -9.3869,  -9.7179,  -9.6270,  -7.9953,  -9.2760,  -9.0413,  -7.1393,\n",
       "          -8.5396,  -8.8955,  -8.4694,  -9.9042,  -8.9546,  -8.7708,  -8.4082,\n",
       "          -8.4481,  -8.2003,  -9.3027,  -9.0164,  -9.0524, -10.0770,  -9.2895,\n",
       "          -7.8214,  -7.2780,  -8.9208, -10.1237,  -9.4625,  -8.8165,  -8.8442,\n",
       "          -8.8269,  -8.3019,  -9.6343,  -9.7509,  -8.9705,  -9.0796,  -8.3672,\n",
       "          -8.8045,  -9.2510,  -9.8967,  -9.6244,  -9.0729,  -9.1109,  -7.7600,\n",
       "          -8.3585,  -9.9215,  -9.5846,  -8.8423,  -8.8221,  -9.5755,  -9.8835,\n",
       "          -8.6362,  -7.7739,  -7.9157,  -9.6111,  -8.6835,  -9.9143,  -8.6399,\n",
       "          -7.8126,  -6.6563,  -8.0145,  -9.9370, -10.5539,  -9.4360,  -9.2563,\n",
       "          -7.5094,  -8.2779,  -8.5072,  -8.9048,  -8.4577,  -9.6545,  -9.1903,\n",
       "          -9.2088,  -8.8763,  -8.5693,  -8.7698,  -9.3954,  -9.2612,  -9.2607,\n",
       "         -10.2539, -10.7391,  -9.5468,  -9.2568,  -8.0793,  -7.4338,  -8.0219,\n",
       "          -8.7450,  -8.2824,  -8.3931, -10.5146,  -8.2424, -10.3647,  -9.3845,\n",
       "          -9.5914,  -9.4364,  -7.7313, -11.0712,  -8.9504,  -7.9431,  -8.7649,\n",
       "          -8.1135,  -8.2482,  -8.4477,  -8.5174,  -9.0736,  -9.2690, -10.5864,\n",
       "         -10.1029,  -8.2630,  -8.9889,  -8.0766,  -7.5987,  -8.9415,  -8.9869,\n",
       "          -7.9498,  -8.7886, -10.0265,  -9.1727,  -8.2280,  -7.2298,  -8.8886,\n",
       "         -10.1581,  -9.3910,  -9.8813,  -9.0176,  -9.4177,  -8.4780,  -9.1111,\n",
       "          -9.1775, -10.2603,  -8.9245,  -9.8161,  -9.6532, -10.1393,  -9.1561,\n",
       "          -7.9431, -10.3570,  -7.5107,  -8.1295,  -8.3676,  -8.7631,  -8.4104,\n",
       "          -8.5459, -11.3416, -10.7921,  -9.6030,  -9.8920,  -9.4376,  -8.5593,\n",
       "          -9.3967, -10.6130,  -9.8908,  -9.0442,  -9.1049,  -7.8753,  -8.2511,\n",
       "          -9.3113,  -9.2495,  -8.1407,  -8.9720, -10.4470, -10.4485,  -9.1358,\n",
       "          -8.4114,  -8.6262,  -9.5968,  -8.2695,  -9.0207,  -8.3457, -10.1416,\n",
       "          -8.8037,  -9.4109, -10.1630,  -8.3965,  -8.9234,  -9.1358,  -8.9047,\n",
       "          -9.1921,  -8.7761,  -8.4995,  -8.3379,  -8.4476,  -9.6656,  -7.2818,\n",
       "          -8.3930,  -7.8818,  -9.8451,  -8.8402,  -9.9259,  -8.5334,  -8.8409,\n",
       "          -9.9644,  -8.0250,  -9.4410,  -9.9227,  -8.2289,  -6.9385,  -9.3434,\n",
       "          -9.7038,  -9.5512,  -8.5088,  -9.0562,  -8.2190,  -9.8215,  -8.6720,\n",
       "          -9.3531,  -8.4354,  -8.5529,  -9.7463,  -8.9260,  -9.2884,  -9.1088,\n",
       "          -7.9462,  -8.6098,  -7.3060,  -8.5993,  -8.1149,  -8.5859,  -8.9447,\n",
       "          -8.9727,  -9.1653, -10.4212,  -8.1489,  -9.4634, -10.3331,  -8.8491,\n",
       "          -9.2567,  -8.4541,  -8.3868,  -8.2190,  -9.5993,  -8.9274,  -8.0239,\n",
       "          -8.1999,  -8.5237,  -9.0582, -10.9024,  -8.2212,  -9.1783,  -7.7487,\n",
       "          -8.6335, -10.0154,  -8.8443,  -9.1093, -10.2248, -11.0282,  -8.2920,\n",
       "          -9.3155,  -8.7914,  -8.4614,  -9.4090,  -8.4092,  -9.1948,  -9.0924,\n",
       "          -9.2706,  -8.9109,  -9.3789, -10.3418,  -9.4435, -10.2213,  -8.4924,\n",
       "          -9.2517,  -8.0221,  -8.8230, -10.1486,  -9.6776,  -8.9743,  -9.7522,\n",
       "          -9.4786,  -8.8933,  -7.8010, -10.1558,  -7.7151,  -7.8095,  -8.4266,\n",
       "          -8.8854,  -8.8435,  -8.6342,  -9.9496,  -8.4425,  -8.6286, -10.7955,\n",
       "          -9.4171, -10.4502,  -9.6028, -10.1359,  -9.8008,  -8.4485,  -8.5916,\n",
       "          -8.5891, -10.0938,  -8.2321,  -7.2973,  -9.4935,  -9.8188,  -8.6800,\n",
       "          -7.2918,  -9.3604,  -9.6684,  -8.7788,  -8.3535,  -9.9416,  -9.5753,\n",
       "          -8.9140,  -8.5611,  -9.0776,  -7.7788,  -8.3663,  -9.3291,  -9.9808,\n",
       "          -8.8947,  -9.7798,  -8.6195, -10.0493,  -9.7083,  -8.5047,  -8.9927,\n",
       "          -8.9326,  -8.1003,  -8.8577,  -9.8789,  -8.0498, -10.2557,  -9.5313,\n",
       "          -7.7701,  -9.8228,  -9.0977,  -6.9533, -10.2494,  -7.1983, -10.8488,\n",
       "          -9.0249,  -9.5360,  -8.7758,  -8.1409,  -9.1713,  -8.0242,  -9.3409,\n",
       "          -8.9066,  -9.0035,  -8.5320,  -9.1465,  -9.1425,  -8.6406,  -8.6088,\n",
       "          -7.8065,  -9.0546,  -9.4377,  -9.0772,  -8.7612,  -8.5377,  -9.8190,\n",
       "          -8.1908,  -9.4934,  -7.8184,  -9.4513,  -9.3401,  -8.4931,  -8.9007,\n",
       "          -8.7387,  -7.9886,  -8.3456,  -8.6788,  -8.7951,  -9.6033,  -7.9731,\n",
       "          -8.7066,  -8.8843,  -7.8266,  -8.7382,  -8.2723,  -8.8423,  -8.6266,\n",
       "          -8.6244,  -7.7398,  -9.9792, -10.2123, -10.4321,  -8.5045,  -8.0867,\n",
       "          -7.5727,  -8.9064,  -8.8766,  -7.8004,  -9.0335,  -9.5063,  -8.5914,\n",
       "          -8.0162,  -7.4803,  -8.3629,  -8.9591,  -8.9700,  -8.5217,  -8.6049,\n",
       "         -10.3821,  -8.8408, -10.3208,  -8.8404, -10.4272, -10.2240,  -8.3108,\n",
       "          -8.1225,  -9.1502,  -8.8122,  -7.8843,  -9.1649,  -9.9464,  -8.2326,\n",
       "          -9.7148,  -7.4200, -10.3929,  -9.5466,  -8.5686, -10.3583,  -7.7396,\n",
       "          -8.5010, -10.9916, -10.3775,  -8.5644,  -8.4161,  -8.4124, -10.2401,\n",
       "          -8.9114,  -8.3439,  -8.0179,  -9.9261,  -8.1556,  -9.0467,  -9.9267,\n",
       "         -10.2797,  -9.2048,  -8.8634,  -7.4749,  -7.3110,  -8.2196,  -9.9659,\n",
       "          -7.4639,  -9.1402,  -8.8369,  -8.5939,  -8.7057,  -7.4697,  -8.5943,\n",
       "          -9.9675,  -8.9917,  -8.4727,  -8.0511,  -7.9592,  -7.5865,  -9.8667,\n",
       "          -9.7037,  -8.3924,  -9.8222,  -9.5598,  -9.3285,  -8.1665,  -9.4453,\n",
       "          -7.9700,  -8.2063,  -9.1728,  -9.8618,  -8.8606,  -9.1347,  -9.7007,\n",
       "          -7.9599,  -9.7538,  -8.1340, -10.6860,  -8.2378, -10.4797, -11.5703,\n",
       "          -8.8172,  -7.7537,  -9.5503,  -9.6728,  -7.2310,  -8.0878,  -9.5992,\n",
       "          -8.1838,  -7.9537,  -9.4510,  -9.3611,  -9.8000,  -9.5767, -10.3686,\n",
       "          -8.7846,  -9.7618,  -9.2231,  -8.3583,  -9.2035,  -9.9208,  -9.7913,\n",
       "          -8.2343,  -8.6454,  -7.2147,  -7.4134, -10.3032, -10.0444,  -7.3795,\n",
       "          -8.2175,  -8.3188,  -9.4062,  -9.8059,  -7.9926,  -9.9610,  -8.4362,\n",
       "          -8.0313,  -8.4489,  -8.8332,  -9.5042, -11.0329,  -9.7885,  -9.1746,\n",
       "          -9.1109,  -8.5238,  -9.1117,  -9.4550,  -8.3034, -10.1111,  -8.6891,\n",
       "          -9.6618, -10.1582, -10.5710,  -9.9951,  -9.3202,  -7.6073,  -9.7648,\n",
       "          -9.3789,  -8.8978, -11.1220,  -9.2198, -10.0956,  -9.4138,  -9.3287,\n",
       "          -9.7714,  -9.3581,  -9.9916, -10.0893,  -9.7747,  -9.3697, -10.1668,\n",
       "         -10.2883, -10.8334,  -8.9749,  -8.5669,  -9.8438,  -9.1781, -10.0614,\n",
       "         -10.0680,  -9.2512,  -8.6355,  -9.9212,  -9.8903,  -9.9014,  -8.9740,\n",
       "         -10.5004,  -8.9119,  -9.0582,  -9.9112, -10.1350, -10.8906,  -9.4633,\n",
       "          -8.8626,  -9.7711,  -8.4782,  -9.9109,  -9.4341,  -8.1434,  -9.7019,\n",
       "          -9.8471, -11.3028,  -8.3165, -11.0223,  -8.8026,  -9.0391, -10.1522,\n",
       "          -9.8476,  -9.2237,  -8.5985,  -9.7812, -10.1832, -10.5293, -12.0252,\n",
       "          -7.7627,  -9.4637, -10.1573,  -9.5862, -10.7458, -10.4971, -11.6923,\n",
       "         -10.3074, -10.0032,  -9.4765, -10.0524,  -8.2213,  -8.0918]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sigkernel\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from itertools import product\n",
    "import cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_obs = np.array([[[2],[2],[3]]]) #Needa 1 Dim, 3 timesteps, 1 output per timestep\n",
    "x_for = np.array([[[2],[100],[3]],[[2],[2],[3]]]) #2 Models, each with 3 timesteps, and 1 output per timestep\n",
    "\n",
    "#if one of the models is the same as the path, value doesn't adjust\n",
    "\n",
    "y_obs = np.cumsum(y_obs, axis=1)\n",
    "x_for = np.cumsum(x_for, axis=1)\n",
    "\n",
    "X = torch.tensor(x_for, dtype=torch.double)\n",
    "y = torch.tensor(y_obs, dtype=torch.double)\n",
    "\n",
    "#y_obs = np.cumsum(y_obs)\n",
    "#y_for = np.cumsum(y_for)\n",
    "\n",
    "static_kernel = sigkernel.RBFKernel(sigma=2)\n",
    "dyadic_order = 4\n",
    "signature_kernel = sigkernel.SigKernel(static_kernel, dyadic_order)\n",
    "\n",
    "K_XX = signature_kernel.compute_Gram(X,X)\n",
    "\n",
    "\n",
    "#sr = signature_kernel.compute_scoring_rule(X,y)\n",
    "#sr\n",
    "\n",
    "# \"\"\"Input:\n",
    "#             - X: torch tensor of shape (batch, length_X, dim),\n",
    "#             - y: torch tensor of shape (1, length_Y, dim)\n",
    "#     Output:\n",
    "#             - signature kernel scoring rule S(X,y) = E[k(X,X)] - 2E[k(X,y]\n",
    "# \"\"\"\n",
    "\n",
    "# K_XX = self.compute_Gram(X, X, sym=True, max_batch=max_batch)\n",
    "# K_Xy = self.compute_Gram(X, y, sym=False, max_batch=max_batch)\n",
    "\n",
    "# K_XX_m = (torch.sum(K_XX) - torch.sum(torch.diag(K_XX))) / (K_XX.shape[0] * (K_XX.shape[0] - 1.))\n",
    "\n",
    "# return K_XX_m - 2. * torch.mean(K_Xy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.3451, 2.2806, 5.3451, 5.3451],\n",
      "        [2.2806, 8.2111, 2.2806, 2.2806],\n",
      "        [5.3451, 2.2806, 5.3451, 5.3451],\n",
      "        [5.3451, 2.2806, 5.3451, 5.3451]], dtype=torch.float64)\n",
      "tensor([[5.3451],\n",
      "        [2.2806],\n",
      "        [5.3451],\n",
      "        [5.3451]], dtype=torch.float64)\n",
      "K_XX_m: tensor(3.8129, dtype=torch.float64)\n",
      "tensor(-9.1580, dtype=torch.float64)\n",
      "tensor(-5.3451, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y_obs = np.array([[[2],[1],[5]]]) #Needa 1 Dim, 3 timesteps, 1 output per timestep\n",
    "x_for = np.array([[[2],[1],[5]],[[2],[99],[500]],[[2],[1],[5]], [[2],[1],[5]]]) #2 Models, each with 3 timesteps, and 1 output per timestep\n",
    "\n",
    "#if one of the models is the same as the path, value doesn't adjust\n",
    "\n",
    "y_obs = np.cumsum(y_obs, axis=1)\n",
    "x_for = np.cumsum(x_for, axis=1)\n",
    "\n",
    "X = torch.tensor(x_for, dtype=torch.double)\n",
    "y = torch.tensor(y_obs, dtype=torch.double)\n",
    "\n",
    "#print(\"X:\", X)\n",
    "#print(\"y:\", y)\n",
    "\n",
    "static_kernel = sigkernel.RBFKernel(sigma=3)\n",
    "dyadic_order = 2\n",
    "signature_kernel = sigkernel.SigKernel(static_kernel, dyadic_order)\n",
    "\n",
    "K_XX = signature_kernel.compute_Gram(X,X)\n",
    "K_Xy = signature_kernel.compute_Gram(X, y)\n",
    "K_XX_m = (torch.sum(K_XX) - torch.sum(torch.diag(K_XX))) / (K_XX.shape[0] * (K_XX.shape[0] - 1.))\n",
    "print(K_XX)\n",
    "print(K_Xy) #\n",
    "print(\"K_XX_m:\", K_XX_m) #Expected self similirity of predictions... \n",
    "print(-2. * torch.mean(K_Xy)) #Expected similarity between prediction and observation\n",
    "print(K_XX_m -(2.*torch.mean(K_Xy)))\n",
    "#Rewards internal consistency + accuracy\n",
    "#Relatively low S(X,y) indicate predicted paths are consistent\n",
    "\n",
    "#Sometimes multiple paths lead to same score when increase to internal similarity first term increases, but if it make it more accurate second term increases as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[[  2.],\n",
      "         [102.],\n",
      "         [105.]],\n",
      "\n",
      "        [[  2.],\n",
      "         [  4.],\n",
      "         [  7.]],\n",
      "\n",
      "        [[  2.],\n",
      "         [  4.],\n",
      "         [  7.]]], dtype=torch.float64)\n",
      "y: tensor([[[2.],\n",
      "         [4.],\n",
      "         [7.]]], dtype=torch.float64)\n",
      "X,y tensor([[[[  4.,   8.,  14.],\n",
      "          [204., 408., 714.],\n",
      "          [210., 420., 735.]]],\n",
      "\n",
      "\n",
      "        [[[  4.,   8.,  14.],\n",
      "          [  8.,  16.,  28.],\n",
      "          [ 14.,  28.,  49.]]],\n",
      "\n",
      "\n",
      "        [[[  4.,   8.,  14.],\n",
      "          [  8.,  16.,  28.],\n",
      "          [ 14.,  28.,  49.]]]], dtype=torch.float64)\n",
      "X,X tensor([[[[4.0000e+00, 2.0400e+02, 2.1000e+02],\n",
      "          [2.0400e+02, 1.0404e+04, 1.0710e+04],\n",
      "          [2.1000e+02, 1.0710e+04, 1.1025e+04]],\n",
      "\n",
      "         [[4.0000e+00, 8.0000e+00, 1.4000e+01],\n",
      "          [2.0400e+02, 4.0800e+02, 7.1400e+02],\n",
      "          [2.1000e+02, 4.2000e+02, 7.3500e+02]],\n",
      "\n",
      "         [[4.0000e+00, 8.0000e+00, 1.4000e+01],\n",
      "          [2.0400e+02, 4.0800e+02, 7.1400e+02],\n",
      "          [2.1000e+02, 4.2000e+02, 7.3500e+02]]],\n",
      "\n",
      "\n",
      "        [[[4.0000e+00, 2.0400e+02, 2.1000e+02],\n",
      "          [8.0000e+00, 4.0800e+02, 4.2000e+02],\n",
      "          [1.4000e+01, 7.1400e+02, 7.3500e+02]],\n",
      "\n",
      "         [[4.0000e+00, 8.0000e+00, 1.4000e+01],\n",
      "          [8.0000e+00, 1.6000e+01, 2.8000e+01],\n",
      "          [1.4000e+01, 2.8000e+01, 4.9000e+01]],\n",
      "\n",
      "         [[4.0000e+00, 8.0000e+00, 1.4000e+01],\n",
      "          [8.0000e+00, 1.6000e+01, 2.8000e+01],\n",
      "          [1.4000e+01, 2.8000e+01, 4.9000e+01]]],\n",
      "\n",
      "\n",
      "        [[[4.0000e+00, 2.0400e+02, 2.1000e+02],\n",
      "          [8.0000e+00, 4.0800e+02, 4.2000e+02],\n",
      "          [1.4000e+01, 7.1400e+02, 7.3500e+02]],\n",
      "\n",
      "         [[4.0000e+00, 8.0000e+00, 1.4000e+01],\n",
      "          [8.0000e+00, 1.6000e+01, 2.8000e+01],\n",
      "          [1.4000e+01, 2.8000e+01, 4.9000e+01]],\n",
      "\n",
      "         [[4.0000e+00, 8.0000e+00, 1.4000e+01],\n",
      "          [8.0000e+00, 1.6000e+01, 2.8000e+01],\n",
      "          [1.4000e+01, 2.8000e+01, 4.9000e+01]]]], dtype=torch.float64)\n",
      "3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "diag(): Supports 1D or 2D tensors. Got 4D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX,X\u001b[39m\u001b[38;5;124m\"\u001b[39m, K_XX)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(K_XX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m K_XX_m \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39msum(K_XX) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK_XX\u001b[49m\u001b[43m)\u001b[49m)) \u001b[38;5;241m/\u001b[39m (K_XX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (K_XX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.\u001b[39m))\n\u001b[0;32m     26\u001b[0m sig_kernel \u001b[38;5;241m=\u001b[39m sigkernel\u001b[38;5;241m.\u001b[39mSigKernel(testkernel, dyadic_order)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: diag(): Supports 1D or 2D tensors. Got 4D"
     ]
    }
   ],
   "source": [
    "y_obs = np.array([[[2],[2],[3]]]) #Needa 1 Dim, 3 timesteps, 1 output per timestep\n",
    "x_for = np.array([[[2],[100],[3]],[[2],[2],[3]],[[2],[2],[3]]]) #2 Models, each with 3 timesteps, and 1 output per timestep\n",
    "\n",
    "#if one of the models is the same as the path, value doesn't adjust\n",
    "\n",
    "y_obs = np.cumsum(y_obs, axis=1)\n",
    "x_for = np.cumsum(x_for, axis=1)\n",
    "\n",
    "X = torch.tensor(x_for, dtype=torch.double)\n",
    "y = torch.tensor(y_obs, dtype=torch.double)\n",
    "\n",
    "print(\"X:\", X)\n",
    "print(\"y:\", y)\n",
    "\n",
    "testkernel = sigkernel.LinearKernel()\n",
    "K_Xy = testkernel.Gram_matrix(X,y)\n",
    "K_XX = testkernel.Gram_matrix(X,X)\n",
    "print(\"X,y\", K_Xy)\n",
    "print(\"X,X\", K_XX)\n",
    "\n",
    "print(K_XX.shape[0])\n",
    "\n",
    "K_XX_m = (torch.sum(K_XX) - torch.sum(torch.diag(K_XX))) / (K_XX.shape[0] * (K_XX.shape[0] - 1.))\n",
    "\n",
    "\n",
    "sig_kernel = sigkernel.SigKernel(testkernel, dyadic_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_t = int(y_obs[0][0])\n",
    "dim_total = y_obs[0].shape[0]-1\n",
    "stat_obs, stat_sim = self._calculate_summary_stat(y_obs, y_sim)\n",
    "\n",
    "# Reshape and compute cumulative sums of statistics\n",
    "\n",
    "stat_obs = np.dstack([np.cumsum(i[1:].reshape(int(dim_total/dim_t),int(dim_t)).T,axis=0) for i in stat_obs])\n",
    "stat_sim = np.dstack([np.cumsum(i[1:].reshape(int(dim_total/dim_t),int(dim_t)).T,axis=0) for i in stat_sim])\n",
    "\n",
    "\n",
    "\n",
    "stat_obs = rearrange(stat_obs, 't b d -> d t b')\n",
    "stat_sim = rearrange(stat_sim, 't b d -> d t b')\n",
    "\n",
    "# # need (obs,time,snp) shape\n",
    "\n",
    "# # Tuning parameter for RBF kernel\n",
    "# \n",
    "static_kernel = sigkernel.RBFKernel(sigma=self.sigma) # hilbert kernel uplift\n",
    "dyadic_order = 3\n",
    "\n",
    "\n",
    "signature_kernel = sigkernel.SigKernel(static_kernel, dyadic_order)\n",
    "\n",
    "# compute the scoring rule\n",
    "\n",
    "sr = signature_kernel.compute_scoring_rule(torch.tensor(stat_sim, dtype=torch.float64),torch.tensor(stat_obs, dtype=torch.float64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
